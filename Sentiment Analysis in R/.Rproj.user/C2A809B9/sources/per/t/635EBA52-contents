---
title: "Sentiment analysis the tidytext way"
author: "Kuo-Lin Tang"
date: "12/05/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Load required packages
```{r}
library(tidyverse)
library(tidytext)
```


# Lexicon-based Sentiment Analysis with tidytext dictionaries
## afinn dictionary
```{r}
afinn_lex = get_sentiments("afinn")
afinn_counts = afinn_lex %>%
  count(value)

# Plot n vs. sentiment
ggplot(afinn_counts, aes(x = value, y = n)) +
  # Add a col layer
  geom_col()
```

## nrc dictionary
```{r}
nrc_lex = get_sentiments("nrc")
nrc_counts = nrc_lex %>%
  count(sentiment)

# Plot n vs. sentiment
ggplot(nrc_counts, aes(x = sentiment, y = n)) +
  # Add a col layer
  geom_col()
```


## loughran dictionary
```{r}
loughran_lex = get_sentiments("loughran")
loughran_counts = loughran_lex %>%
  count(sentiment)

# Plot n vs. sentiment
ggplot(loughran_counts, aes(x = sentiment, y = n)) +
  # Add a col layer
  geom_col()
```


## bing dictionary
```{r}
bing_lex = get_sentiments("bing")
bing_counts = bing_lex %>%
  count(sentiment)

# Plot n vs. sentiment
ggplot(bing_counts, aes(x = sentiment, y = n)) +
  # Add a col layer
  geom_col()
```



# 利用 inner_join() 將我的目標文件和 tidytext 的 dictionaries 內的字連結
## Load the data
```{r}
reviews = read_csv("Datasets/Roomba Reviews.csv")
```

## Data Preparation
### Add an id for each row
```{r}
( reviews = reviews %>%
  mutate(id = as.numeric(row_number())) )
```

### Tokenisation
```{r}
( tokenised_reviews = reviews %>%
  unnest_tokens(term, Review) )
```


### Remove stop words
```{r}
( clean_tokens = tokenised_reviews %>%
  anti_join(stop_words, by = c("term" = "word")) )
```


## Inner_join() with the sentiment dictionary
```{r}
# get the bing lexicon
bing = get_sentiments("bing")

# inner_join()
labelled_tokens = clean_tokens %>%
  inner_join(bing, by = c("term" = "word"))

# Count the sentiment
labelled_tokens %>%
  count(sentiment)
```


```{r}
# Count the sentiment for each document
( labelled_tokens %>%
  count(sentiment, id) )

( wider_labelled_tokens = labelled_tokens %>%
  count(sentiment, id) %>%
  spread(sentiment, n, fill = 0)  )


```














