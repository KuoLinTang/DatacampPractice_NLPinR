knitr::opts_chunk$set(echo = TRUE)
library(tidytext)
library(tidyverse)
amazon_reviews <- stream_in(
gzfile("reviews_Automotive_5.json.gz")
)
library(jsonlite)
amazon_reviews <- stream_in(
gzfile("reviews_Automotive_5.json.gz")
)
head(amazon_reviews)
str(amazon_reviews)
View(amazon_reviews)
amazon_reviews$review_id <- 1:nrow(amazon_reviews)
# 設定自己的stop words vector (移除數字以及car)
my_stopwords = c("car", seq(1:10000))
# 將自己的stop words vector轉換成stop words tibble
my_stopwords_df = tibble(word = my_stopwords, lexicon = "custom")
# 結合自己與預設的stop words dictionary
total_stopwords <- stop_words %>%
bind_rows(my_stopwords_df)
View(amazon_reviews)
tokenised_reviews = amazon_reviews %>%
unnest_tokens(term, reviewText) %>%
anti_join(total_stopwords, by = c("term" = "word"))
View(tokenised_reviews)
tokenised_reviews %>%
group_by(review_id) %>%
summarise(no_stopword_review = paste(term, collapse = " "),
n = n())
reviews_without_stopwords = tokenised_reviews %>%
group_by(review_id) %>%
summarise(no_stopword_review = paste(term, collapse = " "),
total_word_number = n())
View(reviews_without_stopwords)
# 保留字數介於5到250之間的reviews
reviews_without_stopwords = reviews_without_stopwords %>%
filter(total_word_number > 5 & total_word_number < 250)
help(bind_tf_idf)
tfidf_stopwords = tokenised_reviews %>%
count(term, review_id) %>%
bind_tf_idf(term, review_id, n) %>%
filter(tf_idf < 0.001) %>%
mutate(lexicon="tfidf") %>%
select(word,lexicon) %>%
distinct()
tfidf_stopwords = tokenised_reviews %>%
count(term, review_id) %>%
bind_tf_idf(term, review_id, n) %>%
filter(tf_idf < 0.001) %>%
mutate(word = term, lexicon="tfidf") %>%
select(word,lexicon) %>%
distinct()
View(tfidf_stopwords)
tfidf_stopwords = amazon_reviews %>%
unnest_tokens(term, reviewText) %>%
count(term, review_id) %>%
bind_tf_idf(term, review_id, n) %>%
filter(tf_idf < 0.001) %>%
mutate(word = term, lexicon="tfidf") %>%
select(word,lexicon) %>%
distinct()
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidytext)
book = data.frame(chapter = c(1, 2), text = c("It is what it is", "What goes around comes around."))
book %>%
unnest_tokens(input = text,
output = word,
tokens = "words",
format = "text",
drop = T,
to_lower = T)
book %>%
unnest_tokens(input = text,
output = word,
token = "words",
format = "text",
drop = T,
to_lower = T)
book %>%
unnest_tokens(input = text,
output = word,
token = "words",
format = "text",
drop = T,
to_lower = T) %>%
count(chapter, word)
book %>%
unnest_tokens(input = text,
output = word,
token = "words",
format = "text",
drop = T,
to_lower = T) %>%
count(chapter, word) %>%
top_n(1, n)
book %>%
unnest_tokens(input = text,
output = word,
token = "words",
format = "text",
drop = T,
to_lower = T) %>%
count(chapter, word) %>%
group_by(chapter) %>%
top_n(1, n)
dtm = book %>%
unnest_tokens(input = text,
output = word,
token = "words",
format = "text",
drop = T,
to_lower = T) %>%
count(chapter, word) %>%
cast_dtm(document = chapter, term = word, value = n)
View(dtm)
as.matrix(dtm)
LDA_model1 = LDA(x = dtm,
k = 2,
method = "Gibbs",
control = list(alpha = 1,
delta = 0.1,
seed = 42))
library(topicmodels)
LDA_model1 = LDA(x = dtm,
k = 2,
method = "Gibbs",
control = list(alpha = 1,
delta = 0.1,
seed = 42))
LDA_model$beta
LDA_model1$beta
tidy(LDA_model1, matrix = "gamma") %>%
ggplot(aes(x = document, y = gamma)) %>%
geom_col(aes(fill = as.factor(topic)))
tidy(LDA_model1, matrix = "gamma") %>%
ggplot(aes(x = document, y = gamma)) +
geom_col(aes(fill = as.factor(topic)))
tidy(LDA_model1, matrix = "gamma") %>%
ggplot(aes(x = document, y = gamma)) +
geom_col(aes(fill = as.factor(topic)),
position = position_dodge())
tidy(LDA_model1, matrix = "beta") %>%
ggplot(aes(x = document, y = beta)) +
geom_col(aes(fill = as.factor(topic)),
position = position_dodge())
tidy(LDA_model1, matrix = "beta") %>%
ggplot(aes(x = term, y = beta)) +
geom_col(aes(fill = as.factor(topic)),
position = position_dodge())
tidy(LDA_model1, matrix = "beta") %>%
ggplot(aes(x = term, y = beta)) +
geom_col(aes(fill = as.factor(topic)),
position = position_dodge()) +
theme(axis.text = element_text(angle = 90))
tidy(LDA_model1, matrix = "beta") %>%
mutate(topic = as.factor(topic))
ggplot(aes(x = term, y = beta)) +
geom_col(aes(fill = as.factor(topic)),
position = position_dodge()) +
theme(axis.text = element_text(angle = 90))
tidy(LDA_model1, matrix = "beta") %>%
mutate(topic = as.factor(topic))
ggplot(aes(x = term, y = beta)) +
geom_col(aes(fill = topic),
position = position_dodge()) +
theme(axis.text = element_text(angle = 90))
tidy(LDA_model1, matrix = "beta") %>%
mutate(topic = as.factor(topic))
ggplot(aes(x = term, y = beta)) +
geom_col(aes(fill = topic),
position = position_dodge()) +
theme(axis.text.x = element_text(angle = 90))
tidy(LDA_model1, matrix = "beta") %>%
mutate(topic = as.factor(topic)) %>%
ggplot(aes(x = term, y = beta)) +
geom_col(aes(fill = topic),
position = position_dodge()) +
theme(axis.text.x = element_text(angle = 90))
knitr::opts_chunk$set(echo = TRUE)
dtm = book %>%
unnest_tokens(word, text) %>%
?cast_dtm
dtm = book %>%
unnest_tokens(word, text) %>%
count(chapter, word) %>%
cast_dtm(term = word, document = chapter, value = n)
library(topicmodels)
book = data.frame(chapter = c(1, 2), text = c("It is what it is", "What goes around comes around."))
# Build the document-term matrix
dtm = book %>%
unnest_tokens(word, text) %>%
count(chapter, word) %>%
cast_dtm(term = word, document = chapter, value = n)
# LDA modeling
LDA_model = LDA(x = dtm,
k = 2,
method = "Gibbs",
control = list(alpha = 1,
delta = 0.1,
seed = 42,
iter = 1000,
thin = 1))
# Term()
term(LDA_model, k = 5)
# Terms()
terms(LDA_model, k = 5)
terms(LDA_model, threshold = 0.05)
View(dtm)
word_freq = tidy(LDA_model, matrix = "beta") %>%
mutate(n = trunc(beta*10000))
wordcloud(words = word_freq$term,
freq = word_freq$n,
colors = c("DarkOrange", "CornflowerBlue", "DarkRed"),
rot.per = 0.3)
library(wordcloud)
word_freq = tidy(LDA_model, matrix = "beta") %>%
mutate(n = trunc(beta*10000))
wordcloud(words = word_freq$term,
freq = word_freq$n,
colors = c("DarkOrange", "CornflowerBlue", "DarkRed"),
rot.per = 0.3)
history = read_csv("Datasets/history_2.RData")
View(history)
history = read_csv("Datasets/history_2.RData")
history = load("Datasets/history_2.RData")
load("Datasets/history_2.RData")
View(byzantium_clean)
View(byzantium_raw)
dtm = byzantium_clean %>%
unnest_tokens(input = text, output = word) %>%
anti_join(stop_words) %>%
count(chapter, word) %>%
cast_dtm(document = chapter, term = word, value = n)
mod <- LDA(dtm, method = "Gibbs", k = 4,
control=list(alpha=1, seed=10005))
terms(mode, k=15)
terms(mod, k=15)
verbs
View(ner_text)
View(word_counts)
View(byzantium_raw)
past = read_csv("Datasets/Past Tense.xlsx")
View(past)
past = read_csv("Datasets/Past Tense.csv")
past = read_csv("Datasets/Past Tense.csv", header=None)
past = read.csv("Datasets/Past Tense.csv", header = F)
View(past)
class(past$V1)
a = past$V1
past = read.csv("Datasets/Past Tense.csv", header = F) %>%
unnest_tokens(word, V1)
past = past[-c(1,2),]
4562/3
View(ner_text)
knitr::opts_chunk$set(echo = TRUE)
load("Datasets/ch3.RData")
View(df)
load("Datasets/ch3.RData")
dtm = df %>%
unnest_tokens(input = Abstract, output = word) %>%
anti_join(stop_words) %>%
count(AwardNumber, word) %>%
group_by(word) %>%
# Filter for corpus wide frequency
filter(sum(n) >= 10) %>%
ungroup() %>%
cast_dtm(document=AwardNumber, term=word, value=n)
mod = LDA(x=dtm, method="Gibbs", k=3,
control=list(alpha=0.5, seed=1234, iter=500, thin=1))
# Retrieve log-likelihood
logLik(mod)
# Find perplexity
perplexity(object=mod, newdata=dtm)
mod_perplexity = numeric(10)
for (i in 2:10){
model = LDA(x=dtm, method="Gibbs", k=i,
control=list(alpha=0.5, seed=1234, iter=500, thin=1))
mod_perplexity[i] = perplexity(object=model, newdata = dtm)
}
plot(x = c(2:10), y = mod_perplexity, xlab="number of clusters, k",
ylab="perplexity score", type="o")
plot(x = 2:10, y = mod_perplexity, xlab="number of clusters, k",
ylab="perplexity score", type="o")
mod_perplexity = numeric(9)
for (i in 2:10){
model = LDA(x=dtm, method="Gibbs", k=i,
control=list(alpha=0.5, seed=1234, iter=500, thin=1))
mod_perplexity[i] = perplexity(object=model, newdata = dtm)
}
plot(x = 2:10, y = mod_perplexity, xlab="number of clusters, k",
ylab="perplexity score", type="o")
mod_perplexity
plot(x = 1:10, y = mod_perplexity, xlab="number of clusters, k",
ylab="perplexity score", type="o")
plot(x = 2:10, y = mod_perplexity[2:10], xlab="number of clusters, k",
ylab="perplexity score", type="o")
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidytext)
library(topicmodels)
data_airbnb_url = "https://raw.githubusercontent.com/kwartler/text_mining/master/bos_airbnb_1k.csv"
data_airbnb = readr::read_csv(data_airbnb_url) %>%
mutate(doc_id = row_number())
library(textcat)
# automatically set to latin or ASCII
data_airbnb$comments = iconv(data_airbnb$comments)
# 辨認語言
data_airbnb$language = textcat(data_airbnb$comments)
# 檢視各語言的分布
table(data_airbnb$language)
data_airbnb = data_airbnb %>%
filter(language == "english") %>%
rename(documents = comments) %>%
select(review_id,documents,review_scores_rating) %>%
na.omit()
library(udpipe)
langmodel_download = udpipe::udpipe_download_model("english")
langmodel <- udpipe::udpipe_load_model("english-ewt-ud-2.5-191206.udpipe")
postagged = udpipe_annotate(langmodel,
data_airbnb$documents,
parallel.cores = 8,
trace = 1000)
postagged = as.data.frame(postagged)
View(postagged)
lemmatized = postagged %>%
filter(upos %in% c("NOUN", "ADJ", "ADV")) %>%
select(doc_id, lemma) %>%
group_by(doc_id) %>%
summarise(documents_pos_tagged = paste(lemma, collapse = " "))
View(lemmatized)
data_airbnb = data_airbnb %>%
mutate(doc_id = paste0("doc", row_number()))
data_airbnb = data_airbnb %>%
left_join(lemmatized)
View(data_airbnb)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidytext)
load("Datasets/ch3.RData")
dtm = df %>%
unnest_tokens(input = Abstract, output = word) %>%
anti_join(stop_words) %>%
count(AwardNumber, word) %>%
group_by(word) %>%
# Filter for corpus wide frequency
filter(sum(n) >= 10) %>%
ungroup() %>%
cast_dtm(document=AwardNumber, term=word, value=n)
mod = LDA(x=dtm, method="Gibbs", k=3,
control=list(alpha=0.5, seed=1234, iter=500, thin=1))
library(tidyverse)
library(tidytext)
library(topicmodels)
load("Datasets/ch3.RData")
dtm = df %>%
unnest_tokens(input = Abstract, output = word) %>%
anti_join(stop_words) %>%
count(AwardNumber, word) %>%
group_by(word) %>%
# Filter for corpus wide frequency
filter(sum(n) >= 10) %>%
ungroup() %>%
cast_dtm(document=AwardNumber, term=word, value=n)
mod = LDA(x=dtm, method="Gibbs", k=3,
control=list(alpha=0.5, seed=1234, iter=500, thin=1))
# Retrieve log-likelihood
logLik(mod)
# Find perplexity
perplexity(object=mod, newdata=dtm)
mod_perplexity = numeric(9)
for (i in 2:10){
model = LDA(x=dtm, method="Gibbs", k=i,
control=list(alpha=0.5, seed=1234, iter=500, thin=1))
mod_perplexity[i] = perplexity(object=model, newdata = dtm)
}
plot(x = 2:10, y = mod_perplexity[2:10], xlab="number of clusters, k",
ylab="perplexity score", type="o")
